# TGI (Text Generation Inference) production configuration
# Optimized for Phi-3-mini-4k with 8-16k context

# Model configuration
MODEL_ID=microsoft/Phi-3-mini-4k-instruct
REVISION=main

# Performance tuning
MAX_CONCURRENT_REQUESTS=32
MAX_BEST_OF=2
MAX_STOP_SEQUENCES=4
MAX_INPUT_TOKENS=8192
MAX_TOTAL_TOKENS=16384
MAX_BATCH_PREFILL_TOKENS=8192
MAX_BATCH_TOTAL_TOKENS=65536

# Memory optimization
CUDA_MEMORY_FRACTION=0.9
TRUST_REMOTE_CODE=true

# Latency optimization
PORT=8080
HOSTNAME=0.0.0.0
SHARD=false
NUM_SHARD=1

# Quantization (for smaller models, disable for Phi-3-mini)
QUANTIZE=

# Logging and monitoring
LOG_LEVEL=INFO
JSON_OUTPUT=true

# Health checks and metrics
CORS_ALLOW_ORIGIN=*
METRICS_PORT=3000

# Router configuration
MAX_WAITING_TOKENS=20
WAITING_SERVED_RATIO=1.2
MAX_BATCH_SIZE=32

# Advanced tuning for production
PREFILL_CHUNKING=true
SPECULATIVE_TOKENS=0

# Disable features that increase latency
WATERMARK_GAMMA=0.5
WATERMARK_DELTA=2.0