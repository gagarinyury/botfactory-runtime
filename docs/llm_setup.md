# LLM Setup Guide

–†—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ –Ω–∞—Å—Ç—Ä–æ–π–∫–µ –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ LLM –º–æ–¥—É–ª—è —Å 3B –º–æ–¥–µ–ª—å—é –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–æ–≤ –±–æ—Ç–æ–≤.

## –û–±–∑–æ—Ä

LLM –º–æ–¥—É–ª—å –¥–æ–±–∞–≤–ª—è–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ NLU (Natural Language Understanding) –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–æ–≤ –≤ runtime —Å –ø–æ–º–æ—â—å—é –ª–æ–∫–∞–ª—å–Ω–æ–π 3B –º–æ–¥–µ–ª–∏:

- **–ú–æ–¥–µ–ª—å**: Microsoft Phi-3-mini (3.8B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)
- **API**: OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π HTTP –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
- **–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å**: ~30-60 —Ç–æ–∫–µ–Ω–æ–≤/—Å–µ–∫ –Ω–∞ consumer GPU
- **–ö–æ–Ω—Ç–µ–∫—Å—Ç**: 4K —Ç–æ–∫–µ–Ω–æ–≤
- **–ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ**: Redis —Å TTL 15 –º–∏–Ω—É—Ç
- **Rate limiting**: 10 –∑–∞–ø—Ä–æ—Å–æ–≤/–º–∏–Ω—É—Ç—É –Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è

## –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç

### 1. –ó–∞–ø—É—Å–∫ LLM —Å–µ—Ä–≤–∏—Å–∞

```bash
# –ü–æ–¥–Ω—è—Ç—å LLM —Å–µ—Ä–≤–∏—Å
docker compose -f docker-compose.llm.yml up -d llm

# –î–æ–∂–¥–∞—Ç—å—Å—è –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ (–º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–∏–Ω—É—Ç –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏)
docker logs -f botfactory-llm

# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∑–¥–æ—Ä–æ–≤—å–µ —Å–µ—Ä–≤–∏—Å–∞
curl http://localhost:11434/health
```

### 2. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ API

```bash
# –¢–µ—Å—Ç chat completions
curl -s http://localhost:11434/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-3-mini-4k-instruct",
    "messages": [
      {"role": "user", "content": "–ü—Ä–∏–≤–µ—Ç! –ö–∞–∫ –¥–µ–ª–∞?"}
    ],
    "max_tokens": 100,
    "temperature": 0.2
  }'

# –¢–µ—Å—Ç completions
curl -s http://localhost:11434/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-3-mini-4k-instruct",
    "prompt": "–ü–µ—Ä–µ–≤–µ–¥–∏ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π: –ü—Ä–∏–≤–µ—Ç –º–∏—Ä",
    "max_tokens": 50
  }'
```

### 3. –í–∫–ª—é—á–µ–Ω–∏–µ LLM –≤ runtime

```bash
# –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
export LLM_ENABLED=true
export LLM_BASE_URL=http://llm:11434
export LLM_MODEL=microsoft/Phi-3-mini-4k-instruct

# –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç—å runtime
docker compose restart runtime
```

## –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è

### –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è

```bash
# –û—Å–Ω–æ–≤–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
LLM_ENABLED=true                           # –í–∫–ª—é—á–∏—Ç—å/–≤—ã–∫–ª—é—á–∏—Ç—å LLM
LLM_BASE_URL=http://llm:11434             # URL LLM —Å–µ—Ä–≤–∏—Å–∞
LLM_MODEL=microsoft/Phi-3-mini-4k-instruct # ID –º–æ–¥–µ–ª–∏

# –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
LLM_TIMEOUT=30                            # –¢–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–æ–≤ (—Å–µ–∫)
LLM_MAX_RETRIES=3                         # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–≤—Ç–æ—Ä–æ–≤

# –ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è)
LLM_RATE_LIMIT=10                         # –ó–∞–ø—Ä–æ—Å–æ–≤/–º–∏–Ω—É—Ç—É –Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
LLM_CACHE_TTL=900                         # TTL –∫—ç—à–∞ (—Å–µ–∫)
```

### –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏

```yaml
# docker-compose.llm.yml
services:
  llm:
    environment:
      # –ë—ã—Å—Ç—Ä–∞—è –º–æ–¥–µ–ª—å (2B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)
      - MODEL_ID=google/gemma-2b-it

      # –ú—É–ª—å—Ç–∏—è–∑—ã—á–Ω–∞—è –º–æ–¥–µ–ª—å (3B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)
      - MODEL_ID=Qwen/Qwen2.5-3B-Instruct

      # –°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å (3.8B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
      - MODEL_ID=microsoft/Phi-3-mini-4k-instruct
```

## –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤ –±–æ—Ç–∞—Ö

### 1. –£–ª—É—á—à–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤ –≤ action.reply_template.v1

```json
{
  "type": "action.reply_template.v1",
  "params": {
    "text": "–í–∞—à –∑–∞–∫–∞–∑ {{order_id}} –≥–æ—Ç–æ–≤ –∫ –≤—ã–¥–∞—á–µ",
    "llm_improve": true,
    "keyboard": [
      {"text": "–°–ø–∞—Å–∏–±–æ!", "callback": "/thanks"}
    ]
  }
}
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç**: "–û—Ç–ª–∏—á–Ω–æ! –í–∞—à –∑–∞–∫–∞–∑ {{order_id}} –≥–æ—Ç–æ–≤ –∫ –≤—ã–¥–∞—á–µ! üéâ –ú–æ–∂–µ—Ç–µ –∑–∞–±–∏—Ä–∞—Ç—å –≤ —É–¥–æ–±–Ω–æ–µ –≤—Ä–µ–º—è."

### 2. –£–º–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –≤–≤–æ–¥–∞

```python
from runtime.llm_client import llm_client
from runtime.llm_prompts import BotPromptConfigs

# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å –≤–≤–æ–¥–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
config = BotPromptConfigs.smart_validate_input(
    user_input="–∑–∞–≤—Ç—Ä–∞ –≤ 3 –¥–Ω—è",
    expectation="–¥–∞—Ç–∞ –∏ –≤—Ä–µ–º—è –≤ —Ñ–æ—Ä–º–∞—Ç–µ YYYY-MM-DD HH:MM"
)

response = await llm_client.complete(**config)
result = LLMPrompts.parse_json_response(response.content)

if not result["valid"]:
    print(f"–û—à–∏–±–∫–∞: {result['reason']}")
    print(f"–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ: {result['suggestion']}")
```

### 3. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö –º–µ–Ω—é

```python
# –°–æ–∑–¥–∞—Ç—å –º–µ–Ω—é –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
config = BotPromptConfigs.generate_dynamic_menu(
    context="–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –∏–Ω—Ç–µ—Ä–µ—Å—É–µ—Ç—Å—è —Å–ø–∞-–ø—Ä–æ—Ü–µ–¥—É—Ä–∞–º–∏",
    topic="—É—Å–ª—É–≥–∏ —Å–∞–ª–æ–Ω–∞ –∫—Ä–∞—Å–æ—Ç—ã"
)

response = await llm_client.complete(**config)
menu_items = LLMPrompts.parse_json_response(response.content)["items"]

# –†–µ–∑—É–ª—å—Ç–∞—Ç: [
#   {"text": "üíÜ –ú–∞—Å—Å–∞–∂ –ª–∏—Ü–∞", "description": "–†–∞—Å—Å–ª–∞–±–ª—è—é—â–∏–π –º–∞—Å—Å–∞–∂"},
#   {"text": "üßñ –£—Ö–æ–¥ –∑–∞ –∫–æ–∂–µ–π", "description": "–ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–∞—è —á–∏—Å—Ç–∫–∞"},
#   ...
# ]
```

### 4. –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –Ω–∞–º–µ—Ä–µ–Ω–∏–π

```python
# –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —á—Ç–æ —Ö–æ—á–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å
config = BotPromptConfigs.classify_user_intent(
    message="—Ö–æ—á—É –∑–∞–ø–∏—Å–∞—Ç—å—Å—è –Ω–∞ –∑–∞–≤—Ç—Ä–∞ –∫ –º–∞—Å—Ç–µ—Ä—É",
    available_intents=["book_service", "cancel_booking", "get_info", "help"]
)

response = await llm_client.complete(**config)
intent_result = LLMPrompts.parse_json_response(response.content)

print(f"–ù–∞–º–µ—Ä–µ–Ω–∏–µ: {intent_result['intent']}")  # book_service
print(f"–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {intent_result['confidence']}")  # 0.95
print(f"–°—É—â–Ω–æ—Å—Ç–∏: {intent_result['entities']}")  # {"time": "–∑–∞–≤—Ç—Ä–∞"}
```

## –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥

### –ú–µ—Ç—Ä–∏–∫–∏ Prometheus

```
# –ó–∞–ø—Ä–æ—Å—ã
llm_requests_total{type="chat_completion",status="success"}
llm_requests_total{type="completion",status="failed"}

# –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
llm_latency_ms{type="chat_completion",cached="false"}
llm_latency_ms{type="completion",cached="true"}

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤
llm_tokens_total{model="microsoft/Phi-3-mini-4k-instruct",type="input"}
llm_tokens_total{model="microsoft/Phi-3-mini-4k-instruct",type="output"}

# –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ
llm_cache_hits_total{model="microsoft/Phi-3-mini-4k-instruct"}

# –û—à–∏–±–∫–∏
llm_errors_total{model="microsoft/Phi-3-mini-4k-instruct",error_type="timeout"}
llm_errors_total{model="microsoft/Phi-3-mini-4k-instruct",error_type="rate_limit_exceeded"}
```

### –õ–æ–≥–∏

```bash
# –ü—Ä–æ—Å–º–æ—Ç—Ä –ª–æ–≥–æ–≤ LLM —Å–µ—Ä–≤–∏—Å–∞
docker logs botfactory-llm

# –õ–æ–≥–∏ runtime —Å LLM —Å–æ–±—ã—Ç–∏—è–º–∏
docker logs botfactory-runtime | grep llm_

# –û—Å–Ω–æ–≤–Ω—ã–µ —Å–æ–±—ã—Ç–∏—è:
# - llm_request: –Ω–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –∫ LLM
# - llm_success: —É—Å–ø–µ—à–Ω—ã–π –æ—Ç–≤–µ—Ç
# - llm_cache_hit: –ø–æ–ø–∞–¥–∞–Ω–∏–µ –≤ –∫—ç—à
# - llm_text_improved: —Ç–µ–∫—Å—Ç —É–ª—É—á—à–µ–Ω
# - llm_rate_limit_exceeded: –ø—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç
```

## Troubleshooting

### LLM —Å–µ—Ä–≤–∏—Å –Ω–µ –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è

```bash
# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ª–æ–≥–∏
docker logs botfactory-llm

# –¢–∏–ø–∏—á–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã:
# 1. –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–∞–º—è—Ç–∏ (–Ω—É–∂–Ω–æ –º–∏–Ω–∏–º—É–º 4GB RAM)
# 2. –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª–∞—Å—å (–ø—Ä–æ–≤–µ—Ä–∏—Ç—å MODEL_ID)
# 3. –ü—Ä–æ–±–ª–µ–º—ã —Å —Å–µ—Ç—å—é (–ø—Ä–æ–≤–µ—Ä–∏—Ç—å –ø–æ—Ä—Ç—ã)

# –†–µ—à–µ–Ω–∏–µ: —É–≤–µ–ª–∏—á–∏—Ç—å memory limit
docker compose -f docker-compose.llm.yml up -d --scale llm=1 \
  --memory=6g llm
```

### –ú–µ–¥–ª–µ–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã

```bash
# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU
docker exec botfactory-llm nvidia-smi

# –ï—Å–ª–∏ GPU –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –º–æ–¥–µ–ª—å —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ CPU
# –†–µ—à–µ–Ω–∏–µ: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–µ–Ω—å—à—É—é –º–æ–¥–µ–ª—å
MODEL_ID=google/gemma-2b-it  # –í–º–µ—Å—Ç–æ Phi-3
```

### –û—à–∏–±–∫–∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è

```bash
# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å Redis
docker exec botfactory-redis redis-cli ping

# –û—á–∏—Å—Ç–∏—Ç—å –∫—ç—à LLM
docker exec botfactory-redis redis-cli del "llm:cache:*"
```

### Rate limit –ø—Ä–µ–≤—ã—à–µ–Ω

```bash
# –£–≤–µ–ª–∏—á–∏—Ç—å –ª–∏–º–∏—Ç –∏–ª–∏ –æ—á–∏—Å—Ç–∏—Ç—å —Å—á–µ—Ç—á–∏–∫–∏
docker exec botfactory-redis redis-cli del "llm:ratelimit:*"

# –ò–ª–∏ –∏–∑–º–µ–Ω–∏—Ç—å –ª–∏–º–∏—Ç –≤ –∫–æ–¥–µ (llm_client.py:395)
if current_count >= 20:  # –í–º–µ—Å—Ç–æ 10
```

## –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å

### –ë–µ–Ω—á–º–∞—Ä–∫–∏

| –ú–æ–¥–µ–ª—å | –†–∞–∑–º–µ—Ä | –°–∫–æ—Ä–æ—Å—Ç—å | –ü–∞–º—è—Ç—å | –ö–∞—á–µ—Å—Ç–≤–æ |
|--------|--------|----------|---------|----------|
| Gemma-2B | 2B | 60 tok/s | 3GB | –•–æ—Ä–æ—à–µ–µ |
| Phi-3-mini | 3.8B | 40 tok/s | 4GB | –û—Ç–ª–∏—á–Ω–æ–µ |
| Qwen2.5-3B | 3B | 45 tok/s | 3.5GB | –û—Ç–ª–∏—á–Ω–æ–µ |

### –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è

```yaml
# –î–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Å–∫–æ—Ä–æ—Å—Ç–∏
environment:
  - MODEL_ID=google/gemma-2b-it
  - QUANTIZE=int4
  - MAX_BATCH_TOTAL_TOKENS=4096

# –î–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
environment:
  - MODEL_ID=microsoft/Phi-3-mini-4k-instruct
  - QUANTIZE=gptq
  - MAX_BATCH_TOTAL_TOKENS=8192
```

## –ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å

### –í—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è –∑–∞—â–∏—Ç–∞

- **Rate limiting**: 10 –∑–∞–ø—Ä–æ—Å–æ–≤/–º–∏–Ω—É—Ç—É –Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
- **Input sanitization**: –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª–∏–Ω—ã –ø—Ä–æ–º–ø—Ç–æ–≤
- **Timeout protection**: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Ç–∞–π–º–∞—É—Ç 30 —Å–µ–∫
- **Caching**: –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –ø–æ–≤—Ç–æ—Ä–Ω—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è
- **Error handling**: Graceful fallback –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö

### –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ä—ã

```bash
# –û–≥—Ä–∞–Ω–∏—á–∏—Ç—å –¥–æ—Å—Ç—É–ø –∫ LLM —Å–µ—Ä–≤–∏—Å—É
iptables -A INPUT -p tcp --dport 11434 -s 10.0.0.0/8 -j ACCEPT
iptables -A INPUT -p tcp --dport 11434 -j DROP

# –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ä–µ—Å—É—Ä—Å–æ–≤
docker stats botfactory-llm

# –†–æ—Ç–∞—Ü–∏—è –ª–æ–≥–æ–≤
docker run --log-driver=syslog --log-opt max-size=10m botfactory-llm
```

---

**–ì–æ—Ç–æ–≤–æ!** LLM –º–æ–¥—É–ª—å –ø–æ–ª–Ω–æ—Å—Ç—å—é –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω –∏ –≥–æ—Ç–æ–≤ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é. –ù–∞—á–Ω–∏—Ç–µ —Å –≤–∫–ª—é—á–µ–Ω–∏—è `llm_improve: true` –≤ –≤–∞—à–∏—Ö reply templates.